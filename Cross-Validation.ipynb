{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d076c622",
   "metadata": {},
   "source": [
    "## Dropping Age on further models to see if it truly increases Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c5b8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7971c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "X_train[\"CabinNull?\"] = X_train.Cabin.isnull()\n",
    "X_test[\"CabinNull?\"] = X_test.Cabin.isnull()\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\",\"Age\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\",\"Age\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97aacd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a470d452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7661016949152543"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63409d5",
   "metadata": {},
   "source": [
    "This was a further investigation of Age didn't matter and it seems to hold up with some of our later models. I think that dropping Age is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c67e1f",
   "metadata": {},
   "source": [
    "## Imputing Age through KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a1f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "imputer = KNNImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea582a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "X_train[\"CabinNull?\"] = X_train.Cabin.isnull()\n",
    "X_test[\"CabinNull?\"] = X_test.Cabin.isnull()\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ed712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d116347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a766debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c9bdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694915254237288"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4488e5",
   "metadata": {},
   "source": [
    "## Stratified K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa985849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [0.76666667 0.69747899 0.81512605 0.76470588 0.78991597]\n",
      "Average Cross Validation score :0.7667787114845938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "stratifiedkf=StratifiedKFold(n_splits=5)\n",
    "score=cross_val_score(model,X_train,y_train,cv=stratifiedkf)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec974db",
   "metadata": {},
   "source": [
    "Pros: Runs faster. It is stratified. Won't be a perfect cross-validation check, but does the trick so that you aren't just testing against one subset of the data.\n",
    "\n",
    "Cons: You are only doing K sections. So with k/100% of your data not being used, you might be losing some of your valuable data within each split that will help train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e403e",
   "metadata": {},
   "source": [
    "## Leave P out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4406581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [0.5 1.  1.  ... 0.5 1.  0.5]\n",
      "Average Cross Validation score :0.7842930460774914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut,cross_val_score\n",
    "lpo=LeavePOut(p=2)\n",
    "lpo.get_n_splits(X_train)\n",
    "score=cross_val_score(model,X_train,y_train,cv=lpo)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2bdd7",
   "metadata": {},
   "source": [
    "Pros: All the data samples get used as both training and validation samples\n",
    "\n",
    "Cons: High computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb335704",
   "metadata": {},
   "source": [
    "## Leave One Out Cross-Validation\n",
    "\n",
    "\n",
    "Leave one out cross-validation is a special case of Leave P out cross-validation where P=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d74f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
      "Average Cross Validation score :0.7835570469798657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut,cross_val_score\n",
    "loo=LeaveOneOut()\n",
    "score=cross_val_score(model,X_train,y_train,cv=loo)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea53a3b",
   "metadata": {},
   "source": [
    "## Monte Carlo Cross-Validation (Shuffle Split Cross Validation)\n",
    "\n",
    "The datasets get randomly partitioned into training and validation sets. I think that I like this method the most.\n",
    "\n",
    "Cons: There is the possibility that you get samples that are not selected for either training or validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5239bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross Validation scores:n [0.72625698 0.79329609 0.7877095  0.82122905 0.7877095  0.77653631\n",
      " 0.75418994 0.79888268 0.82122905 0.80446927 0.78212291 0.77094972\n",
      " 0.77094972 0.82681564 0.79329609 0.77094972 0.75977654 0.79329609\n",
      " 0.77094972 0.7877095  0.7877095  0.78212291 0.81564246 0.76536313\n",
      " 0.76536313 0.79888268 0.79329609 0.79888268 0.75977654 0.75418994\n",
      " 0.73184358 0.74301676 0.77094972 0.75418994 0.81005587 0.76536313\n",
      " 0.76536313 0.72067039 0.74301676 0.73743017 0.81005587 0.74860335\n",
      " 0.77094972 0.79329609 0.77094972 0.82122905 0.73184358 0.75418994\n",
      " 0.75418994 0.79888268 0.76536313 0.79329609 0.73184358 0.75977654\n",
      " 0.81005587 0.81564246 0.74301676 0.82122905 0.75418994 0.77094972\n",
      " 0.80446927 0.77653631 0.69832402 0.76536313 0.74301676 0.77653631\n",
      " 0.80446927 0.79329609 0.81005587 0.76536313 0.73184358 0.79888268\n",
      " 0.7877095  0.73743017 0.76536313 0.77653631 0.79329609 0.75418994\n",
      " 0.75418994 0.75977654 0.75418994 0.79329609 0.73184358 0.70949721\n",
      " 0.79329609 0.73184358 0.73743017 0.77653631 0.82681564 0.79888268\n",
      " 0.75418994 0.75418994 0.82681564 0.7877095  0.79329609 0.7877095\n",
      " 0.81564246 0.79888268 0.77094972 0.7877095  0.79888268 0.80446927\n",
      " 0.77094972 0.69832402 0.76536313 0.72067039 0.84357542 0.81005587\n",
      " 0.82122905 0.77653631 0.81564246 0.76536313 0.79329609 0.81005587\n",
      " 0.78212291 0.81564246 0.73743017 0.77094972 0.77653631 0.72625698\n",
      " 0.80446927 0.77094972 0.75977654 0.77653631 0.80446927 0.81564246\n",
      " 0.76536313 0.79329609 0.75418994 0.73743017 0.78212291 0.76536313\n",
      " 0.77653631 0.77653631 0.73743017 0.76536313 0.73184358 0.77653631\n",
      " 0.74301676 0.79888268 0.74301676 0.75977654 0.81564246 0.75977654\n",
      " 0.79888268 0.80446927 0.7877095  0.70949721 0.72067039 0.7877095\n",
      " 0.75977654 0.83240223 0.7877095  0.77653631 0.81005587 0.81005587\n",
      " 0.8547486  0.78212291 0.82122905 0.77094972 0.75418994 0.77653631\n",
      " 0.77094972 0.81564246 0.77094972 0.82681564 0.78212291 0.80446927\n",
      " 0.70391061 0.75977654 0.81005587 0.74860335 0.74301676 0.78212291\n",
      " 0.78212291 0.78212291 0.76536313 0.77653631 0.77653631 0.73184358\n",
      " 0.78212291 0.77094972 0.74301676 0.75418994 0.74860335 0.81564246\n",
      " 0.77653631 0.81005587 0.81005587 0.81564246 0.75418994 0.74860335\n",
      " 0.79888268 0.7877095  0.78212291 0.84357542 0.79888268 0.75977654\n",
      " 0.79329609 0.77094972 0.75977654 0.72625698 0.73743017 0.73743017\n",
      " 0.78212291 0.73743017 0.78212291 0.82122905 0.79329609 0.70391061\n",
      " 0.7877095  0.75418994 0.79329609 0.74860335 0.7150838  0.77653631\n",
      " 0.76536313 0.75418994 0.79888268 0.7877095  0.7877095  0.81564246\n",
      " 0.79888268 0.75418994 0.75418994 0.77094972 0.7877095  0.77094972\n",
      " 0.7877095  0.74301676 0.77653631 0.75418994 0.79888268 0.78212291\n",
      " 0.73184358 0.79888268 0.77653631 0.78212291 0.74860335 0.7877095\n",
      " 0.75418994 0.78212291 0.75418994 0.77094972 0.73743017 0.77094972\n",
      " 0.79329609 0.77094972 0.76536313 0.81005587 0.80446927 0.77653631\n",
      " 0.83798883 0.8547486  0.74860335 0.7877095  0.7877095  0.79329609\n",
      " 0.76536313 0.74301676 0.75977654 0.73743017 0.74301676 0.82681564\n",
      " 0.76536313 0.82122905 0.75977654 0.7877095  0.7877095  0.77653631\n",
      " 0.82122905 0.76536313 0.75977654 0.73184358 0.79888268 0.79888268\n",
      " 0.7877095  0.74860335 0.74860335 0.78212291 0.75977654 0.77094972\n",
      " 0.78212291 0.75418994 0.7877095  0.74860335 0.80446927 0.78212291\n",
      " 0.79329609 0.74301676 0.82122905 0.77653631 0.76536313 0.77094972\n",
      " 0.73743017 0.74301676 0.72625698 0.7877095  0.76536313 0.77094972\n",
      " 0.78212291 0.74860335 0.74860335 0.83798883 0.81005587 0.79329609\n",
      " 0.74860335 0.78212291 0.79329609 0.75977654 0.73743017 0.76536313\n",
      " 0.79888268 0.77653631 0.74301676 0.7877095  0.80446927 0.79329609\n",
      " 0.81564246 0.77653631 0.76536313 0.76536313 0.81564246 0.76536313\n",
      " 0.82681564 0.74860335 0.80446927 0.77653631 0.74301676 0.78212291\n",
      " 0.75977654 0.81005587 0.82681564 0.81005587 0.82122905 0.7877095\n",
      " 0.75977654 0.81564246 0.7877095  0.7877095  0.78212291 0.80446927\n",
      " 0.79888268 0.75977654 0.76536313 0.7877095  0.77653631 0.77653631\n",
      " 0.75418994 0.74860335 0.74301676 0.77653631 0.81005587 0.7877095\n",
      " 0.80446927 0.80446927 0.74301676 0.79329609 0.78212291 0.78212291\n",
      " 0.74301676 0.70949721 0.79888268 0.77094972 0.74860335 0.75418994\n",
      " 0.79888268 0.81005587 0.7877095  0.75418994 0.70391061 0.72067039\n",
      " 0.75418994 0.74301676 0.76536313 0.77094972 0.76536313 0.73184358\n",
      " 0.80446927 0.77653631 0.77653631 0.80446927 0.76536313 0.81564246\n",
      " 0.77094972 0.70391061 0.75418994 0.76536313 0.79888268 0.77653631\n",
      " 0.7877095  0.82681564 0.75418994 0.72625698 0.78212291 0.72625698\n",
      " 0.79888268 0.75418994 0.77653631 0.79888268 0.77094972 0.75977654\n",
      " 0.78212291 0.73743017 0.79329609 0.79329609 0.72067039 0.77094972\n",
      " 0.79329609 0.75977654 0.75977654 0.73743017 0.77653631 0.77653631\n",
      " 0.79329609 0.83798883 0.77094972 0.74860335 0.7877095  0.75977654\n",
      " 0.78212291 0.74301676 0.79329609 0.82122905 0.78212291 0.79329609\n",
      " 0.83240223 0.72625698 0.74860335 0.74860335 0.74301676 0.75977654\n",
      " 0.80446927 0.79888268 0.75418994 0.81564246 0.79888268 0.77653631\n",
      " 0.80446927 0.76536313 0.7877095  0.7877095  0.82122905 0.77653631\n",
      " 0.76536313 0.75977654 0.80446927 0.80446927 0.77653631 0.77653631\n",
      " 0.78212291 0.79329609 0.73743017 0.75418994 0.82122905 0.77094972\n",
      " 0.77094972 0.74301676 0.82122905 0.79888268 0.7877095  0.73743017\n",
      " 0.75418994 0.77094972 0.78212291 0.74301676 0.78212291 0.78212291\n",
      " 0.79888268 0.77094972 0.77094972 0.73743017 0.73743017 0.80446927\n",
      " 0.7877095  0.72625698 0.79888268 0.80446927 0.75418994 0.78212291\n",
      " 0.7150838  0.76536313 0.74301676 0.7877095  0.81564246 0.72625698\n",
      " 0.77094972 0.76536313 0.73743017 0.81005587 0.75977654 0.7877095\n",
      " 0.78212291 0.75977654 0.79329609 0.75977654 0.79329609 0.75977654\n",
      " 0.78212291 0.80446927 0.7150838  0.73743017 0.72067039 0.76536313\n",
      " 0.75977654 0.82122905 0.75977654 0.73184358 0.79888268 0.69832402\n",
      " 0.73184358 0.75418994 0.76536313 0.75418994 0.77653631 0.77653631\n",
      " 0.79329609 0.72067039 0.72625698 0.74301676 0.72067039 0.7877095\n",
      " 0.79329609 0.73184358 0.75977654 0.78212291 0.75977654 0.7877095\n",
      " 0.7877095  0.81005587 0.75977654 0.83798883 0.83798883 0.79888268\n",
      " 0.81005587 0.80446927 0.77653631 0.77653631 0.77094972 0.79329609\n",
      " 0.75977654 0.72067039 0.75977654 0.78212291 0.75977654 0.79329609\n",
      " 0.77653631 0.7877095  0.76536313 0.79329609 0.77094972 0.75977654\n",
      " 0.74860335 0.73743017 0.79888268 0.75977654 0.78212291 0.74301676\n",
      " 0.75977654 0.78212291 0.76536313 0.78212291 0.7877095  0.77653631\n",
      " 0.73743017 0.75977654 0.81564246 0.77653631 0.77653631 0.73184358\n",
      " 0.7877095  0.77653631 0.7877095  0.81005587 0.76536313 0.78212291\n",
      " 0.75418994 0.83798883 0.83240223 0.75977654 0.79888268 0.81564246\n",
      " 0.78212291 0.7877095  0.75977654 0.72067039 0.80446927 0.7877095\n",
      " 0.79888268 0.75418994 0.76536313 0.75418994 0.81564246 0.81564246\n",
      " 0.72625698 0.74301676 0.81005587 0.74860335 0.81564246 0.77094972\n",
      " 0.77653631 0.73743017 0.7877095  0.77653631 0.79888268 0.82681564\n",
      " 0.73743017 0.80446927 0.72067039 0.76536313 0.78212291 0.81005587\n",
      " 0.74301676 0.79888268 0.74301676 0.80446927 0.78212291 0.77653631\n",
      " 0.77653631 0.76536313 0.79329609 0.75977654 0.74301676 0.78212291\n",
      " 0.77094972 0.7877095  0.79888268 0.80446927 0.77094972 0.78212291\n",
      " 0.66480447 0.79888268 0.78212291 0.75977654 0.82681564 0.77094972\n",
      " 0.76536313 0.7150838  0.76536313 0.76536313 0.79888268 0.77653631\n",
      " 0.79888268 0.73184358 0.77094972 0.81005587 0.77094972 0.7877095\n",
      " 0.78212291 0.83240223 0.79329609 0.76536313 0.78212291 0.77094972\n",
      " 0.80446927 0.72067039 0.80446927 0.72625698 0.80446927 0.72067039\n",
      " 0.75977654 0.73743017 0.81564246 0.78212291 0.7877095  0.72625698\n",
      " 0.81005587 0.76536313 0.81564246 0.77653631 0.74860335 0.79888268\n",
      " 0.79888268 0.75418994 0.79329609 0.75977654 0.74860335 0.80446927\n",
      " 0.83798883 0.77094972 0.7877095  0.81005587 0.79888268 0.78212291\n",
      " 0.77094972 0.79888268 0.73184358 0.75977654 0.69273743 0.79888268\n",
      " 0.78212291 0.73743017 0.73184358 0.7877095  0.78212291 0.79888268\n",
      " 0.76536313 0.72625698 0.81564246 0.7877095  0.75977654 0.7877095\n",
      " 0.73743017 0.75977654 0.70949721 0.80446927 0.74301676 0.7150838\n",
      " 0.76536313 0.73743017 0.82122905 0.78212291 0.77653631 0.77653631\n",
      " 0.74860335 0.82681564 0.77094972 0.79329609 0.77653631 0.74301676\n",
      " 0.73743017 0.79888268 0.82122905 0.77094972 0.79329609 0.80446927\n",
      " 0.77653631 0.79329609 0.78212291 0.81564246 0.78212291 0.75977654\n",
      " 0.75418994 0.77653631 0.77094972 0.82122905 0.74301676 0.77094972\n",
      " 0.72067039 0.77094972 0.82122905 0.81564246 0.76536313 0.76536313\n",
      " 0.80446927 0.74860335 0.73743017 0.75977654 0.78212291 0.73743017\n",
      " 0.76536313 0.7877095  0.74301676 0.81005587 0.80446927 0.75418994\n",
      " 0.75418994 0.76536313 0.74860335 0.79888268 0.77094972 0.80446927\n",
      " 0.77653631 0.76536313 0.78212291 0.7877095  0.77653631 0.84357542\n",
      " 0.81005587 0.75418994 0.77653631 0.75977654 0.79888268 0.79329609\n",
      " 0.81005587 0.76536313 0.7877095  0.81005587 0.79329609 0.79329609\n",
      " 0.79329609 0.81005587 0.77653631 0.72067039 0.73184358 0.81005587\n",
      " 0.76536313 0.82122905 0.74301676 0.81564246 0.78212291 0.81564246\n",
      " 0.80446927 0.74301676 0.73743017 0.76536313 0.75418994 0.73743017\n",
      " 0.7150838  0.78212291 0.80446927 0.74860335 0.78212291 0.75418994\n",
      " 0.78212291 0.72625698 0.78212291 0.76536313 0.7877095  0.81564246\n",
      " 0.79329609 0.74301676 0.73184358 0.7877095  0.75977654 0.77094972\n",
      " 0.77653631 0.81005587 0.82122905 0.81564246 0.72625698 0.78212291\n",
      " 0.81005587 0.75418994 0.75418994 0.74301676 0.79329609 0.81005587\n",
      " 0.77094972 0.81564246 0.7877095  0.79888268 0.79329609 0.72625698\n",
      " 0.75418994 0.75418994 0.77653631 0.7150838  0.77094972 0.76536313\n",
      " 0.77653631 0.7877095  0.73743017 0.74301676 0.75977654 0.75977654\n",
      " 0.78212291 0.73743017 0.80446927 0.78212291 0.74860335 0.75418994\n",
      " 0.72625698 0.76536313 0.81005587 0.74860335 0.74860335 0.76536313\n",
      " 0.81564246 0.7877095  0.80446927 0.77094972 0.78212291 0.75418994\n",
      " 0.77653631 0.74860335 0.81005587 0.7877095  0.75977654 0.79888268\n",
      " 0.7877095  0.78212291 0.78212291 0.72625698 0.76536313 0.80446927\n",
      " 0.68715084 0.74860335 0.79329609 0.74301676 0.79888268 0.76536313\n",
      " 0.73184358 0.79888268 0.81564246 0.79888268 0.7877095  0.77653631\n",
      " 0.76536313 0.79329609 0.75418994 0.77094972 0.78212291 0.75977654\n",
      " 0.79329609 0.75977654 0.75977654 0.77653631 0.7877095  0.7877095\n",
      " 0.74301676 0.7877095  0.79329609 0.75977654 0.7877095  0.76536313\n",
      " 0.81005587 0.77653631 0.75977654 0.77094972 0.77653631 0.78212291\n",
      " 0.74860335 0.76536313 0.81564246 0.79888268 0.73743017 0.77094972\n",
      " 0.80446927 0.79329609 0.80446927 0.73184358 0.77094972 0.74860335\n",
      " 0.81005587 0.75977654 0.76536313 0.79888268 0.79329609 0.77653631\n",
      " 0.74860335 0.78212291 0.76536313 0.84357542 0.74301676 0.79329609\n",
      " 0.80446927 0.77094972 0.73743017 0.75418994 0.77094972 0.76536313\n",
      " 0.70949721 0.78212291 0.77094972 0.7877095  0.82122905 0.75418994\n",
      " 0.80446927 0.77094972 0.81005587 0.77094972 0.79329609 0.78212291\n",
      " 0.81564246 0.74860335 0.75977654 0.74301676 0.73743017 0.78212291\n",
      " 0.78212291 0.79329609 0.80446927 0.79888268 0.79329609 0.74301676\n",
      " 0.74301676 0.75418994 0.78212291 0.81564246 0.82122905 0.78212291\n",
      " 0.77094972 0.74860335 0.75977654 0.81564246 0.7877095  0.79888268\n",
      " 0.77653631 0.83240223 0.77094972 0.80446927 0.82122905 0.76536313\n",
      " 0.75418994 0.79888268 0.81005587 0.75977654 0.76536313 0.7877095\n",
      " 0.69832402 0.77094972 0.77094972 0.81005587 0.81005587 0.79888268\n",
      " 0.77653631 0.73743017 0.77653631 0.76536313]\n",
      "Average Cross Validation score :0.7751675977653631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "shuffle_split=ShuffleSplit(test_size=0.3,n_splits=1000)\n",
    "scores=cross_val_score(model,X_train,y_train,cv=shuffle_split)\n",
    "print(\"cross Validation scores:n {}\".format(scores))\n",
    "print(\"Average Cross Validation score :{}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2810af9",
   "metadata": {},
   "source": [
    "## Using GridSearch and Pipelines to hypertune our models (I think I'm jumping the gun a bit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c508b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "splitter = ['best','random']\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d31288d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('dt', DecisionTreeClassifier())]),\n",
       "             param_grid=[{'dt__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90,\n",
       "                                            100, 110, None],\n",
       "                          'dt__max_features': ['auto', 'sqrt', 'log2'],\n",
       "                          'dt__min_samples_leaf': [1, 2, 4],\n",
       "                          'dt__min_samples_split': [2, 5, 10],\n",
       "                          'dt__splitter': ['best', 'random']}])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_pipe = Pipeline(\n",
    "    [('dt', DecisionTreeClassifier())])\n",
    "param_grid = [{'dt__max_features':max_features,\n",
    "              'dt__max_depth':max_depth,\n",
    "              'dt__min_samples_split':min_samples_split,\n",
    "              'dt__min_samples_leaf':min_samples_leaf,\n",
    "              'dt__splitter':splitter}]\n",
    "gs = GridSearchCV(dt_pipe, param_grid)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49ccb2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt__max_depth': 40,\n",
       " 'dt__max_features': 'sqrt',\n",
       " 'dt__min_samples_leaf': 2,\n",
       " 'dt__min_samples_split': 10,\n",
       " 'dt__splitter': 'random'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ac9362f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735593220338983"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 40, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 10, splitter = 'random')\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4529ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross Validation scores:n [0.80446927 0.81564246 0.7877095  0.79888268 0.84357542 0.81564246\n",
      " 0.83240223 0.79329609 0.81564246 0.7877095  0.73184358 0.82681564\n",
      " 0.79329609 0.81564246 0.76536313 0.72067039 0.82681564 0.7877095\n",
      " 0.80446927 0.77094972 0.81564246 0.87150838 0.83798883 0.81005587\n",
      " 0.79329609 0.82681564 0.7877095  0.77094972 0.82122905 0.83240223\n",
      " 0.82122905 0.77094972 0.8603352  0.82681564 0.8547486  0.77094972\n",
      " 0.79888268 0.83240223 0.79888268 0.79888268 0.8603352  0.80446927\n",
      " 0.77094972 0.83798883 0.8547486  0.82122905 0.82122905 0.82122905\n",
      " 0.73743017 0.74301676 0.83798883 0.84357542 0.82122905 0.83798883\n",
      " 0.75977654 0.79888268 0.84916201 0.77653631 0.81564246 0.83798883\n",
      " 0.81005587 0.74301676 0.82681564 0.84916201 0.84357542 0.84357542\n",
      " 0.84357542 0.83240223 0.77653631 0.82681564 0.80446927 0.79329609\n",
      " 0.79888268 0.78212291 0.83240223 0.78212291 0.81005587 0.83798883\n",
      " 0.81564246 0.8547486  0.8547486  0.7877095  0.8547486  0.77653631\n",
      " 0.79329609 0.82122905 0.70391061 0.79888268 0.80446927 0.84357542\n",
      " 0.81005587 0.83240223 0.83798883 0.82681564 0.75418994 0.83240223\n",
      " 0.82681564 0.77653631 0.78212291 0.67597765]\n",
      "Average Cross Validation score :0.8079329608938547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "shuffle_split=ShuffleSplit(test_size=0.3,n_splits=100)\n",
    "scores=cross_val_score(dt,X_train,y_train,cv=shuffle_split)\n",
    "print(\"cross Validation scores:n {}\".format(scores))\n",
    "print(\"Average Cross Validation score :{}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ef439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
