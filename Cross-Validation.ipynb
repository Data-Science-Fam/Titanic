{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d076c622",
   "metadata": {},
   "source": [
    "## Dropping Age on further models to see if it truly increases Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5b8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7971c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "X_train[\"CabinNull?\"] = X_train.Cabin.isnull()\n",
    "X_test[\"CabinNull?\"] = X_test.Cabin.isnull()\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\",\"Age\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\",\"Age\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97aacd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a470d452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7661016949152543"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63409d5",
   "metadata": {},
   "source": [
    "This was a further investigation of Age didn't matter and it seems to hold up with some of our later models. I think that dropping Age is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c67e1f",
   "metadata": {},
   "source": [
    "## Imputing Age through KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a1f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "imputer = KNNImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea582a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "X_train[\"CabinNull?\"] = X_train.Cabin.isnull()\n",
    "X_test[\"CabinNull?\"] = X_test.Cabin.isnull()\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ed712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d116347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a766debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c9bdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694915254237288"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4488e5",
   "metadata": {},
   "source": [
    "## Stratified K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa985849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [0.76666667 0.69747899 0.81512605 0.76470588 0.78991597]\n",
      "Average Cross Validation score :0.7667787114845938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "stratifiedkf=StratifiedKFold(n_splits=5)\n",
    "score=cross_val_score(model,X_train,y_train,cv=stratifiedkf)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec974db",
   "metadata": {},
   "source": [
    "Pros: Runs faster. It is stratified. Won't be a perfect cross-validation check, but does the trick so that you aren't just testing against one subset of the data.\n",
    "\n",
    "Cons: You are only doing K sections. So with k/100% of your data not being used, you might be losing some of your valuable data within each split that will help train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e403e",
   "metadata": {},
   "source": [
    "## Leave P out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4406581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [0.5 1.  1.  ... 0.5 1.  0.5]\n",
      "Average Cross Validation score :0.7842930460774914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut,cross_val_score\n",
    "lpo=LeavePOut(p=2)\n",
    "lpo.get_n_splits(X_train)\n",
    "score=cross_val_score(model,X_train,y_train,cv=lpo)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2bdd7",
   "metadata": {},
   "source": [
    "Pros: All the data samples get used as both training and validation samples\n",
    "\n",
    "Cons: High computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb335704",
   "metadata": {},
   "source": [
    "## Leave One Out Cross-Validation\n",
    "\n",
    "\n",
    "Leave one out cross-validation is a special case of Leave P out cross-validation where P=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d74f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
      "Average Cross Validation score :0.7835570469798657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut,cross_val_score\n",
    "loo=LeaveOneOut()\n",
    "score=cross_val_score(model,X_train,y_train,cv=loo)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea53a3b",
   "metadata": {},
   "source": [
    "## Monte Carlo Cross-Validation (Shuffle Split Cross Validation)\n",
    "\n",
    "The datasets get randomly partitioned into training and validation sets. I think that I like this method the most.\n",
    "\n",
    "Cons: There is the possibility that you get samples that are not selected for either training or validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross Validation scores:n [0.77094972 0.75977654 0.81005587 ... 0.77653631 0.81005587 0.78212291]\n",
      "Average Cross Validation score :0.7749357541899441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "shuffle_split=ShuffleSplit(test_size=0.3,n_splits=10000)\n",
    "scores=cross_val_score(model,X_train,y_train,cv=shuffle_split)\n",
    "print(\"cross Validation scores:n {}\".format(scores))\n",
    "print(\"Average Cross Validation score :{}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6066ae",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fbc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "imputer = KNNImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81780afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "X_train[\"CabinNull?\"] = X_train.Cabin.isnull()\n",
    "X_test[\"CabinNull?\"] = X_test.Cabin.isnull()\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf5e3e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    596.000000\n",
       "mean      31.276104\n",
       "std       46.497266\n",
       "min        0.000000\n",
       "25%        7.895800\n",
       "50%       14.054150\n",
       "75%       30.017700\n",
       "max      512.329200\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.Fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae360747",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10611fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a766debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c9bdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694915254237288"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5fb92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2810af9",
   "metadata": {},
   "source": [
    "## Using GridSearch and Pipelines to hypertune our models (I think I'm jumping the gun a bit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c508b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "splitter = ['best','random']\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d31288d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('dt', DecisionTreeClassifier())]),\n",
       "             param_grid=[{'dt__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90,\n",
       "                                            100, 110, None],\n",
       "                          'dt__max_features': ['auto', 'sqrt', 'log2'],\n",
       "                          'dt__min_samples_leaf': [1, 2, 4],\n",
       "                          'dt__min_samples_split': [2, 5, 10],\n",
       "                          'dt__splitter': ['best', 'random']}])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_pipe = Pipeline(\n",
    "    [('dt', DecisionTreeClassifier())])\n",
    "param_grid = [{'dt__max_features':max_features,\n",
    "              'dt__max_depth':max_depth,\n",
    "              'dt__min_samples_split':min_samples_split,\n",
    "              'dt__min_samples_leaf':min_samples_leaf,\n",
    "              'dt__splitter':splitter}]\n",
    "gs = GridSearchCV(dt_pipe, param_grid)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49ccb2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt__max_depth': 40,\n",
       " 'dt__max_features': 'sqrt',\n",
       " 'dt__min_samples_leaf': 2,\n",
       " 'dt__min_samples_split': 10,\n",
       " 'dt__splitter': 'random'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ac9362f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735593220338983"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 40, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 10, splitter = 'random')\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4529ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross Validation scores:n [0.80446927 0.81564246 0.7877095  0.79888268 0.84357542 0.81564246\n",
      " 0.83240223 0.79329609 0.81564246 0.7877095  0.73184358 0.82681564\n",
      " 0.79329609 0.81564246 0.76536313 0.72067039 0.82681564 0.7877095\n",
      " 0.80446927 0.77094972 0.81564246 0.87150838 0.83798883 0.81005587\n",
      " 0.79329609 0.82681564 0.7877095  0.77094972 0.82122905 0.83240223\n",
      " 0.82122905 0.77094972 0.8603352  0.82681564 0.8547486  0.77094972\n",
      " 0.79888268 0.83240223 0.79888268 0.79888268 0.8603352  0.80446927\n",
      " 0.77094972 0.83798883 0.8547486  0.82122905 0.82122905 0.82122905\n",
      " 0.73743017 0.74301676 0.83798883 0.84357542 0.82122905 0.83798883\n",
      " 0.75977654 0.79888268 0.84916201 0.77653631 0.81564246 0.83798883\n",
      " 0.81005587 0.74301676 0.82681564 0.84916201 0.84357542 0.84357542\n",
      " 0.84357542 0.83240223 0.77653631 0.82681564 0.80446927 0.79329609\n",
      " 0.79888268 0.78212291 0.83240223 0.78212291 0.81005587 0.83798883\n",
      " 0.81564246 0.8547486  0.8547486  0.7877095  0.8547486  0.77653631\n",
      " 0.79329609 0.82122905 0.70391061 0.79888268 0.80446927 0.84357542\n",
      " 0.81005587 0.83240223 0.83798883 0.82681564 0.75418994 0.83240223\n",
      " 0.82681564 0.77653631 0.78212291 0.67597765]\n",
      "Average Cross Validation score :0.8079329608938547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "shuffle_split=ShuffleSplit(test_size=0.3,n_splits=100)\n",
    "scores=cross_val_score(dt,X_train,y_train,cv=shuffle_split)\n",
    "print(\"cross Validation scores:n {}\".format(scores))\n",
    "print(\"Average Cross Validation score :{}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "075ef439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>CabinNull?</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7875</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7292</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6500</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>30.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>596 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  SibSp  Parch     Fare  CabinNull?  Pclass_2  Pclass_3  Sex_male  \\\n",
       "496  54.0      1      0  78.2667       False         0         0         0   \n",
       "14   14.0      0      0   7.8542        True         0         1         0   \n",
       "82    NaN      0      0   7.7875        True         0         1         0   \n",
       "657  32.0      1      1  15.5000        True         0         1         0   \n",
       "388   NaN      0      0   7.7292        True         0         1         1   \n",
       "..    ...    ...    ...      ...         ...       ...       ...       ...   \n",
       "715  19.0      0      0   7.6500       False         0         1         1   \n",
       "767  30.5      0      0   7.7500        True         0         1         0   \n",
       "72   21.0      0      0  73.5000        True         1         0         1   \n",
       "235   NaN      0      0   7.5500        True         0         1         0   \n",
       "37   21.0      0      0   8.0500        True         0         1         1   \n",
       "\n",
       "     Embarked_Q  Embarked_S  \n",
       "496           0           0  \n",
       "14            0           1  \n",
       "82            1           0  \n",
       "657           1           0  \n",
       "388           1           0  \n",
       "..          ...         ...  \n",
       "715           0           1  \n",
       "767           1           0  \n",
       "72            0           1  \n",
       "235           0           1  \n",
       "37            0           1  \n",
       "\n",
       "[596 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
