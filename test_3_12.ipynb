{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ebae956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0581af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender=pd.read_csv(\"gender_submission.csv\")\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "train=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbf5b2",
   "metadata": {},
   "source": [
    "# Logistic Imputation of age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f74fe2",
   "metadata": {},
   "source": [
    "The idea here is that since the magnitude of age increases with time (difference between 5-10 yr old is more significant than 65-70 yr old), a logistic regression might be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c4fe7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#adding log of Age\n",
    "X_train[\"Age\"] = X_train[\"Age\"].apply(np.log)\n",
    "X_test[\"Age\"] = X_test[\"Age\"].apply(np.log)\n",
    "\n",
    "#Binning - Fare\n",
    "X_train['Fare_bin'] = pd.cut(X_train['Fare'], bins=[np.percentile(X_train.Fare,0),np.percentile(X_train.Fare,33.3),np.percentile(X_train.Fare,66.6),\n",
    "                              np.percentile(X_train.Fare,100)], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "X_test['Fare_bin'] = pd.cut(X_train['Fare'], bins=[np.percentile(X_train.Fare,0),np.percentile(X_train.Fare,33.3),np.percentile(X_train.Fare,66.6),\n",
    "                              np.percentile(X_train.Fare,100)], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41484220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    596.000000\n",
       "mean       3.219256\n",
       "std        0.743836\n",
       "min       -0.400478\n",
       "25%        3.044522\n",
       "50%        3.357861\n",
       "75%        3.658407\n",
       "max        4.382027\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.Age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d9bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "001f6960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (596, 11)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 1.41\n",
      "[IterativeImputer] Change: 2.151003515095147, scaled tolerance: 0.5123292 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 2.82\n",
      "[IterativeImputer] Change: 0.18878348870863215, scaled tolerance: 0.5123292 \n",
      "[IterativeImputer] Early stopping criterion reached.\n",
      "[IterativeImputer] Completing matrix with shape (295, 11)\n",
      "[IterativeImputer] Ending imputation round 1/2, elapsed time 0.01\n",
      "[IterativeImputer] Ending imputation round 2/2, elapsed time 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "imp = IterativeImputer(estimator=rf,missing_values=np.nan, max_iter=10, verbose=2, imputation_order='roman',random_state=1)\n",
    "X_train=pd.DataFrame(imp.fit_transform(X_train),columns = X_train.columns)\n",
    "X_test=pd.DataFrame(imp.transform(X_test),columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e7f32090",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1652/1772563324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchi2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbestfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdfscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdfcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "bestfeatures = SelectKBest(score_func = chi2, k = 10)\n",
    "fit = bestfeatures.fit(X_train, y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis = 1)\n",
    "featureScores.columns = ['Specs','Score']\n",
    "featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "174eb8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "462080a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186440677966102"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82807fda",
   "metadata": {},
   "source": [
    "Monte Carlo Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "750475de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross Validation scores:n [0.81564246 0.78212291 0.83240223 0.83798883 0.75418994 0.80446927\n",
      " 0.78212291 0.80446927 0.77094972 0.79888268 0.77653631 0.7877095\n",
      " 0.81005587 0.77094972 0.79329609 0.77094972 0.77653631 0.73743017\n",
      " 0.75977654 0.78212291 0.77094972 0.77653631 0.7877095  0.81564246\n",
      " 0.77653631 0.77653631 0.75418994 0.77094972 0.76536313 0.82122905\n",
      " 0.72625698 0.74860335 0.79888268 0.77653631 0.82681564 0.75977654\n",
      " 0.7877095  0.77094972 0.74301676 0.76536313 0.78212291 0.77653631\n",
      " 0.78212291 0.80446927 0.79329609 0.79329609 0.81005587 0.75977654\n",
      " 0.7877095  0.79329609 0.82681564 0.73743017 0.78212291 0.74860335\n",
      " 0.79888268 0.81005587 0.78212291 0.79329609 0.75977654 0.82122905\n",
      " 0.77653631 0.77094972 0.78212291 0.76536313 0.76536313 0.74860335\n",
      " 0.81005587 0.77094972 0.77653631 0.7877095  0.76536313 0.77094972\n",
      " 0.74860335 0.75977654 0.7877095  0.81564246 0.79888268 0.82681564\n",
      " 0.81005587 0.74860335 0.75977654 0.7877095  0.75977654 0.75977654\n",
      " 0.76536313 0.7877095  0.81005587 0.81005587 0.77094972 0.76536313\n",
      " 0.7877095  0.76536313 0.79329609 0.80446927 0.77094972 0.78212291\n",
      " 0.80446927 0.81005587 0.77094972 0.77094972 0.78212291 0.83240223\n",
      " 0.76536313 0.82681564 0.80446927 0.79888268 0.77094972 0.81564246\n",
      " 0.82681564 0.77653631 0.74301676 0.70949721 0.81005587 0.76536313\n",
      " 0.77094972 0.73184358 0.7877095  0.79329609 0.75977654 0.75418994\n",
      " 0.7877095  0.81005587 0.83240223 0.74860335 0.84357542 0.78212291\n",
      " 0.82122905 0.79329609 0.75977654 0.74301676 0.77094972 0.75977654\n",
      " 0.74301676 0.77094972 0.75977654 0.79888268 0.77094972 0.81005587\n",
      " 0.75977654 0.80446927 0.80446927 0.8547486  0.77094972 0.81005587\n",
      " 0.77653631 0.78212291 0.76536313 0.7150838  0.77094972 0.79888268\n",
      " 0.84357542 0.78212291 0.7877095  0.77094972 0.75418994 0.81005587\n",
      " 0.79329609 0.78212291 0.7877095  0.75418994 0.81564246 0.75418994\n",
      " 0.77094972 0.73743017 0.7877095  0.77653631 0.76536313 0.76536313\n",
      " 0.7877095  0.77653631 0.77094972 0.84357542 0.74860335 0.80446927\n",
      " 0.7877095  0.74860335 0.76536313 0.80446927 0.78212291 0.78212291\n",
      " 0.78212291 0.79329609 0.79888268 0.77094972 0.72625698 0.73743017\n",
      " 0.82681564 0.73743017 0.81005587 0.77094972 0.7877095  0.75977654\n",
      " 0.73743017 0.77653631 0.83240223 0.75418994 0.75977654 0.79329609\n",
      " 0.77094972 0.75977654 0.74301676 0.83240223 0.77094972 0.82122905\n",
      " 0.7877095  0.79329609 0.83240223 0.84916201 0.76536313 0.82122905\n",
      " 0.72067039 0.77094972 0.73184358 0.78212291 0.81005587 0.79888268\n",
      " 0.77653631 0.79329609 0.77653631 0.78212291 0.74860335 0.81564246\n",
      " 0.79329609 0.75418994 0.78212291 0.75418994 0.75977654 0.74301676\n",
      " 0.77094972 0.73184358 0.73743017 0.74860335 0.76536313 0.81005587\n",
      " 0.80446927 0.79329609 0.79329609 0.77094972 0.74860335 0.79888268\n",
      " 0.75418994 0.82122905 0.78212291 0.74860335 0.81564246 0.80446927\n",
      " 0.80446927 0.80446927 0.73743017 0.80446927 0.7877095  0.76536313\n",
      " 0.77094972 0.73743017 0.81564246 0.76536313 0.82122905 0.75977654\n",
      " 0.74301676 0.79329609 0.73743017 0.73184358 0.77094972 0.80446927\n",
      " 0.83240223 0.77653631 0.79888268 0.77094972 0.77653631 0.81005587\n",
      " 0.82681564 0.77653631 0.74860335 0.76536313 0.74301676 0.80446927\n",
      " 0.79888268 0.77653631 0.73743017 0.77653631 0.7877095  0.77094972\n",
      " 0.72067039 0.81005587 0.7877095  0.81005587 0.75418994 0.81564246\n",
      " 0.79329609 0.74860335 0.70949721 0.79888268 0.70949721 0.74860335\n",
      " 0.75418994 0.7877095  0.7877095  0.78212291 0.79888268 0.81005587\n",
      " 0.80446927 0.80446927 0.75418994 0.76536313 0.75418994 0.75977654\n",
      " 0.80446927 0.79888268 0.84357542 0.77094972 0.74860335 0.80446927\n",
      " 0.79329609 0.78212291 0.74860335 0.81564246 0.78212291 0.7877095\n",
      " 0.77094972 0.77094972 0.79329609 0.75418994 0.80446927 0.76536313\n",
      " 0.75977654 0.75418994 0.79888268 0.83240223 0.81005587 0.77094972\n",
      " 0.75977654 0.79888268 0.79888268 0.80446927 0.78212291 0.82681564\n",
      " 0.74860335 0.81005587 0.78212291 0.79888268 0.82122905 0.78212291\n",
      " 0.73743017 0.81564246 0.81564246 0.79888268 0.82122905 0.82122905\n",
      " 0.79329609 0.79888268 0.79888268 0.79329609 0.80446927 0.81005587\n",
      " 0.80446927 0.7150838  0.77653631 0.78212291 0.75977654 0.76536313\n",
      " 0.79888268 0.80446927 0.77094972 0.78212291 0.77653631 0.75418994\n",
      " 0.78212291 0.76536313 0.74860335 0.81005587 0.76536313 0.79888268\n",
      " 0.7877095  0.77653631 0.79329609 0.75977654 0.84916201 0.7877095\n",
      " 0.76536313 0.77094972 0.83798883 0.77653631 0.73184358 0.76536313\n",
      " 0.75977654 0.79329609 0.7877095  0.80446927 0.80446927 0.74301676\n",
      " 0.80446927 0.7877095  0.82681564 0.76536313 0.79329609 0.79329609\n",
      " 0.7877095  0.73184358 0.84916201 0.72067039 0.77094972 0.81005587\n",
      " 0.75418994 0.80446927 0.79888268 0.77653631 0.73743017 0.7877095\n",
      " 0.79329609 0.77653631 0.7877095  0.76536313 0.7877095  0.79329609\n",
      " 0.81005587 0.75977654 0.75977654 0.83798883 0.7877095  0.78212291\n",
      " 0.83240223 0.79329609 0.72625698 0.74860335 0.79329609 0.81005587\n",
      " 0.77094972 0.77653631 0.83798883 0.84916201 0.81564246 0.79329609\n",
      " 0.75418994 0.81564246 0.74860335 0.79329609 0.81005587 0.81005587\n",
      " 0.78212291 0.83240223 0.77653631 0.81005587 0.74860335 0.73184358\n",
      " 0.73184358 0.77094972 0.7877095  0.82122905 0.77653631 0.78212291\n",
      " 0.77094972 0.83240223 0.77094972 0.74301676 0.81005587 0.81005587\n",
      " 0.79888268 0.79888268 0.81564246 0.81005587 0.73184358 0.7150838\n",
      " 0.80446927 0.79329609 0.82122905 0.77653631 0.72067039 0.77094972\n",
      " 0.77653631 0.83798883 0.81564246 0.79329609 0.80446927 0.81564246\n",
      " 0.79329609 0.75418994 0.79329609 0.75418994 0.78212291 0.7877095\n",
      " 0.78212291 0.72625698 0.77094972 0.78212291 0.74860335 0.77653631\n",
      " 0.7877095  0.75977654 0.74301676 0.83240223 0.74301676 0.70949721\n",
      " 0.77094972 0.80446927 0.73184358 0.79888268 0.80446927 0.76536313\n",
      " 0.79888268 0.80446927 0.78212291 0.74301676 0.77094972 0.77094972\n",
      " 0.74860335 0.80446927 0.80446927 0.75977654 0.77094972 0.74860335\n",
      " 0.77094972 0.81005587 0.79888268 0.73184358 0.70949721 0.72625698\n",
      " 0.74860335 0.77653631 0.81005587 0.77653631 0.80446927 0.78212291\n",
      " 0.77094972 0.80446927 0.83240223 0.77653631 0.7877095  0.81005587\n",
      " 0.78212291 0.69273743 0.79888268 0.81005587 0.76536313 0.82122905\n",
      " 0.75977654 0.73743017 0.77094972 0.77094972 0.79888268 0.7877095\n",
      " 0.7877095  0.83798883 0.73184358 0.81564246 0.74860335 0.76536313\n",
      " 0.77653631 0.76536313 0.77653631 0.80446927 0.79888268 0.78212291\n",
      " 0.7150838  0.74860335 0.76536313 0.79329609 0.82681564 0.80446927\n",
      " 0.77653631 0.75977654 0.79888268 0.77653631 0.7877095  0.74301676\n",
      " 0.7877095  0.80446927 0.79888268 0.7877095  0.7877095  0.77653631\n",
      " 0.74860335 0.82681564 0.77094972 0.79888268 0.77094972 0.79888268\n",
      " 0.7877095  0.77653631 0.78212291 0.77653631 0.77094972 0.75977654\n",
      " 0.77653631 0.80446927 0.7877095  0.79888268 0.79888268 0.80446927\n",
      " 0.77094972 0.77094972 0.8603352  0.75977654 0.74860335 0.77094972\n",
      " 0.80446927 0.77094972 0.75977654 0.82681564 0.7877095  0.75977654\n",
      " 0.79888268 0.80446927 0.75977654 0.7877095  0.74301676 0.81564246\n",
      " 0.75418994 0.80446927 0.80446927 0.81005587 0.79888268 0.81564246\n",
      " 0.81564246 0.79888268 0.77094972 0.81005587 0.77653631 0.80446927\n",
      " 0.7877095  0.80446927 0.74860335 0.79888268 0.7877095  0.79888268\n",
      " 0.77094972 0.7877095  0.74301676 0.78212291 0.77094972 0.81005587\n",
      " 0.81564246 0.74860335 0.77653631 0.77653631 0.74301676 0.73184358\n",
      " 0.81564246 0.79888268 0.81564246 0.76536313 0.78212291 0.79329609\n",
      " 0.7877095  0.79888268 0.83798883 0.77094972 0.7877095  0.74301676\n",
      " 0.75418994 0.82122905 0.78212291 0.76536313 0.79329609 0.79329609\n",
      " 0.75418994 0.73743017 0.77094972 0.7877095  0.73184358 0.77653631\n",
      " 0.82681564 0.74860335 0.77094972 0.78212291 0.80446927 0.80446927\n",
      " 0.77094972 0.74301676 0.72067039 0.78212291 0.79329609 0.76536313\n",
      " 0.75977654 0.82681564 0.7877095  0.78212291 0.77653631 0.77653631\n",
      " 0.81005587 0.79888268 0.79329609 0.77094972 0.83798883 0.78212291\n",
      " 0.75977654 0.84916201 0.77653631 0.82681564 0.77094972 0.7877095\n",
      " 0.78212291 0.84357542 0.82122905 0.7877095  0.75977654 0.77094972\n",
      " 0.74301676 0.74860335 0.80446927 0.78212291 0.77094972 0.75418994\n",
      " 0.80446927 0.77094972 0.76536313 0.75977654 0.79329609 0.76536313\n",
      " 0.72625698 0.78212291 0.77653631 0.79888268 0.73184358 0.78212291\n",
      " 0.75977654 0.77094972 0.81564246 0.82122905 0.83798883 0.74860335\n",
      " 0.81005587 0.77653631 0.74860335 0.76536313 0.80446927 0.79329609\n",
      " 0.76536313 0.7877095  0.73743017 0.79329609 0.74860335 0.82681564\n",
      " 0.78212291 0.81005587 0.78212291 0.77653631 0.75977654 0.75418994\n",
      " 0.79888268 0.77653631 0.77094972 0.78212291 0.75977654 0.77653631\n",
      " 0.77094972 0.81564246 0.80446927 0.79329609 0.80446927 0.79329609\n",
      " 0.73184358 0.75418994 0.80446927 0.84357542 0.82681564 0.74860335\n",
      " 0.79329609 0.77653631 0.80446927 0.7150838  0.72625698 0.82681564\n",
      " 0.79329609 0.77653631 0.81564246 0.73743017 0.78212291 0.73184358\n",
      " 0.74860335 0.84357542 0.79329609 0.78212291 0.80446927 0.77653631\n",
      " 0.73184358 0.79888268 0.73743017 0.76536313 0.81564246 0.80446927\n",
      " 0.79888268 0.7877095  0.79888268 0.75418994 0.77094972 0.77653631\n",
      " 0.81564246 0.78212291 0.79888268 0.77094972 0.79888268 0.80446927\n",
      " 0.81005587 0.75977654 0.74860335 0.72625698 0.76536313 0.79888268\n",
      " 0.80446927 0.79329609 0.7877095  0.80446927 0.77653631 0.80446927\n",
      " 0.75418994 0.79888268 0.77653631 0.73184358 0.80446927 0.76536313\n",
      " 0.79329609 0.81564246 0.7877095  0.79329609 0.75418994 0.81564246\n",
      " 0.77094972 0.81005587 0.7150838  0.75977654 0.7150838  0.74301676\n",
      " 0.77094972 0.7877095  0.77653631 0.77094972 0.79329609 0.78212291\n",
      " 0.76536313 0.77094972 0.79888268 0.75418994 0.79888268 0.83798883\n",
      " 0.78212291 0.76536313 0.80446927 0.78212291 0.77094972 0.76536313\n",
      " 0.80446927 0.75977654 0.79888268 0.7877095  0.79329609 0.77094972\n",
      " 0.75977654 0.79329609 0.78212291 0.73743017 0.83240223 0.78212291\n",
      " 0.80446927 0.75977654 0.70949721 0.77094972 0.7877095  0.77094972\n",
      " 0.77094972 0.75418994 0.81564246 0.81005587 0.77094972 0.75418994\n",
      " 0.77653631 0.70949721 0.80446927 0.81564246 0.78212291 0.76536313\n",
      " 0.80446927 0.83240223 0.81005587 0.72625698 0.74860335 0.76536313\n",
      " 0.79329609 0.81564246 0.74301676 0.77094972 0.78212291 0.7877095\n",
      " 0.76536313 0.7150838  0.79888268 0.77094972 0.81005587 0.7877095\n",
      " 0.72625698 0.77653631 0.69832402 0.77653631 0.7877095  0.77653631\n",
      " 0.81564246 0.73743017 0.78212291 0.81564246 0.82681564 0.75418994\n",
      " 0.7877095  0.74860335 0.79888268 0.77094972 0.79329609 0.77094972\n",
      " 0.80446927 0.78212291 0.80446927 0.79329609 0.79888268 0.7150838\n",
      " 0.81005587 0.81564246 0.77094972 0.76536313 0.7877095  0.77653631\n",
      " 0.7877095  0.79888268 0.75977654 0.75977654 0.81564246 0.77653631\n",
      " 0.75977654 0.76536313 0.77094972 0.7877095  0.75418994 0.75977654\n",
      " 0.77094972 0.79888268 0.74301676 0.78212291 0.75418994 0.75977654\n",
      " 0.80446927 0.76536313 0.77094972 0.72067039 0.73743017 0.78212291\n",
      " 0.78212291 0.75418994 0.76536313 0.76536313 0.74860335 0.78212291\n",
      " 0.78212291 0.81564246 0.75977654 0.80446927 0.82681564 0.74860335\n",
      " 0.80446927 0.76536313 0.81005587 0.78212291 0.75418994 0.77653631\n",
      " 0.81005587 0.78212291 0.83240223 0.76536313 0.77653631 0.76536313\n",
      " 0.79329609 0.72067039 0.77094972 0.7877095  0.7877095  0.79329609\n",
      " 0.81005587 0.78212291 0.81564246 0.74301676 0.77094972 0.75977654\n",
      " 0.79888268 0.81005587 0.72067039 0.77094972 0.79888268 0.81005587\n",
      " 0.76536313 0.77094972 0.81005587 0.80446927 0.79888268 0.75977654\n",
      " 0.77094972 0.80446927 0.77094972 0.77094972 0.76536313 0.79888268\n",
      " 0.79888268 0.82681564 0.80446927 0.81564246]\n",
      "Average Cross Validation score :0.7814022346368714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "shuffle_split=ShuffleSplit(test_size=0.3,n_splits=1000)\n",
    "scores=cross_val_score(model,X_train,y_train,cv=shuffle_split)\n",
    "print(\"cross Validation scores:n {}\".format(scores))\n",
    "print(\"Average Cross Validation score :{}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79a1f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "imputer = KNNImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ea582a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=\"Survived\"), df[\"Survived\"], test_size = 0.33, random_state=1)\n",
    "\n",
    "X_train[\"CabinNull?\"] = X_train.Cabin.isnull()\n",
    "X_test[\"CabinNull?\"] = X_test.Cabin.isnull()\n",
    "\n",
    "#Drop the unique columns that won't help our model. Both in the test and train datasets\n",
    "X_train.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "X_test.drop(columns = [\"Name\",\"Ticket\", \"PassengerId\",\"Cabin\"], inplace = True)\n",
    "\n",
    "#Impute the numerical variables with mean\n",
    "X_train[\"Fare\"] = X_train[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "X_test[\"Fare\"] = X_test[\"Fare\"].replace(np.NaN, X_train[\"Fare\"].mean())\n",
    "\n",
    "#adding log of Age\n",
    "X_train[\"Age\"] = X_train[\"Age\"].apply(np.log)\n",
    "X_test[\"Age\"] = X_test[\"Age\"].apply(np.log)\n",
    "\n",
    "#Binning - Fare\n",
    "X_train['Fare_bin'] = pd.cut(X_train['Fare'], bins=[np.percentile(X_train.Fare,0),np.percentile(X_train.Fare,33.3),np.percentile(X_train.Fare,66.6),\n",
    "                              np.percentile(X_train.Fare,100)], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "X_test['Fare_bin'] = pd.cut(X_train['Fare'], bins=[np.percentile(X_train.Fare,0),np.percentile(X_train.Fare,33.3),np.percentile(X_train.Fare,66.6),\n",
    "                              np.percentile(X_train.Fare,100)], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "\n",
    "#total family members\n",
    "X_train['total_fam'] = X_train['SibSp'] + X_train['Parch']\n",
    "X_test['total_fam'] = X_test['SibSp'] + X_test['Parch']\n",
    "\n",
    "#Change Passenger class to a string variable instead of numerical\n",
    "X_train.Pclass = X_train.Pclass.astype(str)\n",
    "X_test.Pclass = X_test.Pclass.astype(str)\n",
    "\n",
    "#Impute the Categorical variables\n",
    "#X_train.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "#X_test.Cabin = X_train.Cabin.fillna(X_train['Cabin'].value_counts().index[0])\n",
    "\n",
    "X_train.Embarked = X_train.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "X_test.Embarked = X_test.Embarked.fillna(X_train['Embarked'].value_counts().index[0])\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "42ed712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d116347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "78608d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Specs</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.428833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SibSp</td>\n",
       "      <td>0.078864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parch</td>\n",
       "      <td>3.147825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fare</td>\n",
       "      <td>5.686320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CabinNull?</td>\n",
       "      <td>13.376564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_fam</td>\n",
       "      <td>0.402681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pclass_2</td>\n",
       "      <td>6.653592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pclass_3</td>\n",
       "      <td>30.249451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>67.454910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Embarked_Q</td>\n",
       "      <td>0.042507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Embarked_S</td>\n",
       "      <td>4.486832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fare_bin_Mid</td>\n",
       "      <td>0.468503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fare_bin_High</td>\n",
       "      <td>33.519389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Specs      Score\n",
       "0             Age   0.428833\n",
       "1           SibSp   0.078864\n",
       "2           Parch   3.147825\n",
       "3            Fare   5.686320\n",
       "4      CabinNull?  13.376564\n",
       "5       total_fam   0.402681\n",
       "6        Pclass_2   6.653592\n",
       "7        Pclass_3  30.249451\n",
       "8        Sex_male  67.454910\n",
       "9      Embarked_Q   0.042507\n",
       "10     Embarked_S   4.486832\n",
       "11   Fare_bin_Mid   0.468503\n",
       "12  Fare_bin_High  33.519389"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "bestfeatures = SelectKBest(score_func = chi2, k = 10)\n",
    "fit = bestfeatures.fit(X_train, y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis = 1)\n",
    "featureScores.columns = ['Specs','Score']\n",
    "featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a766debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48c9bdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7966101694915254"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa977ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross Validation scores:n [0.7877095  0.75977654 0.73184358 ... 0.77094972 0.69832402 0.77653631]\n",
      "Average Cross Validation score :0.7723486033519552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "shuffle_split=ShuffleSplit(test_size=0.3,n_splits=10000)\n",
    "scores=cross_val_score(model,X_train,y_train,cv=shuffle_split)\n",
    "print(\"cross Validation scores:n {}\".format(scores))\n",
    "print(\"Average Cross Validation score :{}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b32f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
